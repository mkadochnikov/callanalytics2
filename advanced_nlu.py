#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π NLU –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ —Å–∞–Ω–∞—Ç–æ—Ä–∏—è
"""

import re
import json
import logging
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
from pathlib import Path
import numpy as np

# Natasha - –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π NER –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc
)

# –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç –∏ –¥–µ–Ω–µ–≥
from natasha import DatesExtractor, MoneyExtractor

# DeepPavlov –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from deeppavlov import configs, build_model

    DEEPPAVLOV_AVAILABLE = True
except ImportError:
    DEEPPAVLOV_AVAILABLE = False
    logging.warning("DeepPavlov –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install deeppavlov")

# Transformers –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
from transformers import pipeline
import torch

logger = logging.getLogger(__name__)


class AdvancedNLUAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π NLU –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤"""

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha
        self._init_natasha()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepPavlov (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        self._init_deeppavlov()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self._init_additional_models()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞
        self._load_business_patterns()

    def _init_natasha(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Natasha"""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha...")

        self.segmenter = Segmenter()
        self.morph_vocab = MorphVocab()

        self.emb = NewsEmbedding()
        self.morph_tagger = NewsMorphTagger(self.emb)
        self.syntax_parser = NewsSyntaxParser(self.emb)
        self.ner_tagger = NewsNERTagger(self.emb)

        # –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.dates_extractor = DatesExtractor(self.morph_vocab)
        self.money_extractor = MoneyExtractor(self.morph_vocab)

        logger.info("‚úÖ Natasha –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def _init_deeppavlov(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π DeepPavlov"""
        if not DEEPPAVLOV_AVAILABLE:
            self.intent_model = None
            self.ner_model = None
            return

        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepPavlov...")
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π DeepPavlov –∑–¥–µ—Å—å
            self.intent_model = None
            self.ner_model = None
            logger.info("‚úÖ DeepPavlov –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DeepPavlov: {e}")
            self.intent_model = None
            self.ner_model = None

    def _init_additional_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.dialogue_classifier = None
            logger.info("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except:
            self.dialogue_classifier = None
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")

    def _load_business_patterns(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞"""

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–∏—è
        self.sanatorium_objections = {
            "üè• –õ–µ—á–µ–Ω–∏–µ –Ω–µ –Ω—É–∂–Ω–æ": {
                "patterns": [
                    r"–Ω–µ –Ω—É–∂–Ω–æ –ª–µ—á–µ–Ω–∏–µ",
                    r"—Ç–æ–ª—å–∫–æ –æ—Ç–¥—ã—Ö",
                    r"–±–µ–∑ –ø—Ä–æ—Ü–µ–¥—É—Ä",
                    r"–ø—Ä–æ—Å—Ç–æ –æ—Ç–¥–æ—Ö–Ω—É—Ç—å",
                    r"–±–µ–∑ –ª–µ—á–µ–Ω–∏—è"
                ],
                "recommendation": "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ–∑–¥–æ—Ä–æ–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –±–µ–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä"
            },
            "üìÖ –ù–µ—É–¥–æ–±–Ω—ã–µ –¥–∞—Ç—ã": {
                "patterns": [
                    r"–¥–∞—Ç—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç",
                    r"–≤ –¥—Ä—É–≥–æ–µ –≤—Ä–µ–º—è",
                    r"–ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞",
                    r"–Ω–µ –º–æ–∂–µ–º –≤ —ç—Ç–∏ —á–∏—Å–ª–∞",
                    r"–Ω–µ—É–¥–æ–±–Ω—ã–π –ø–µ—Ä–∏–æ–¥"
                ],
                "recommendation": "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –¥–∞—Ç—ã –∏–ª–∏ –≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ –∑–∞–µ–∑–¥–∞"
            },
            "üí∞ –í—ã—Å–æ–∫–∞—è —Ü–µ–Ω–∞": {
                "patterns": [
                    r"–¥–æ—Ä–æ–≥–æ",
                    r"—Ü–µ–Ω–∞ –≤—ã—Å–æ–∫–∞—è",
                    r"–º–Ω–æ–≥–æ –¥–µ–Ω–µ–≥",
                    r"–Ω–µ –ø–æ –∫–∞—Ä–º–∞–Ω—É",
                    r"—Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ",
                    r"200.*–∑–∞ –∫–∏—Å–ª–æ–≤–æ–¥—Å–∫",
                    r"–º–∞–ª—å–¥–∏–≤—ã –¥–µ—à–µ–≤–ª–µ"
                ],
                "recommendation": "–ü–æ–∫–∞–∑–∞—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∞–∫—Ü–∏–∏"
            }
        }

        # –¢–µ–º—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–∏—è
        self.sanatorium_topics = {
            "–ë—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–±—Ä–æ–Ω—å", "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞—Ç—å", "–Ω–æ–º–µ—Ä —Å–≤–æ–±–æ–¥–µ–Ω"],
            "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–µ–Ω–∞—Ö": ["—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ü–µ–Ω–∞", "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç", "—Ç–∞—Ä–∏—Ñ", "–ø—Ä–∞–π—Å"],
            "–£—Å–ª–æ–≤–∏—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è": ["–ø–∏—Ç–∞–Ω–∏–µ", "–Ω–æ–º–µ—Ä", "—É–¥–æ–±—Å—Ç–≤–∞", "—á—Ç–æ –≤—Ö–æ–¥–∏—Ç", "–≤–∫–ª—é—á–µ–Ω–æ"],
            "–õ–µ—á–µ–±–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã": ["–ª–µ—á–µ–Ω–∏–µ", "–ø—Ä–æ—Ü–µ–¥—É—Ä—ã", "–≤—Ä–∞—á", "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–ø—Ä–æ—Ñ–∏–ª—å"]
        }

        # –≠—Ç–∞–ø—ã –ø—Ä–æ–¥–∞–∂–∏
        self.sales_stages = {
            "initial_contact": "–ü–µ—Ä–≤–∏—á–Ω—ã–π –∫–æ–Ω—Ç–∞–∫—Ç",
            "needs_identification": "–í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π",
            "presentation": "–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è —É—Å–ª—É–≥",
            "objection_handling": "–†–∞–±–æ—Ç–∞ —Å –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏",
            "closing": "–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–¥–µ–ª–∫–∏",
            "post_sale": "–ü–æ—Å—Ç–ø—Ä–æ–¥–∞–∂–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ"
        }

    def analyze_dialogue(self, transcript: str, call_info: Dict = None) -> Dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞"""

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏
        dialogue_turns = self._split_dialogue(transcript)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Natasha
        entities = self._extract_entities_natasha(transcript)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã
        intents = self._analyze_intents_fallback(dialogue_turns)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        topic = self._determine_topic(transcript, intents)

        # –ù–∞—Ö–æ–¥–∏–º –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è
        objections = self._find_objections(transcript, dialogue_turns)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç—Ç–∞–ø –ø—Ä–æ–¥–∞–∂–∏
        sales_stage = self._determine_sales_stage(dialogue_turns, intents)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        call_result = self._analyze_call_result(transcript, dialogue_turns, objections)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –±–∏–∑–Ω–µ—Å-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        business_data = self._extract_business_data(entities, transcript)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        emotional_context = self._analyze_emotional_context(dialogue_turns)

        return {
            "topic": topic,
            "objections": objections,
            "entities": entities,
            "intents": intents,
            "sales_stage": sales_stage,
            "call_result": call_result,
            "business_data": business_data,
            "emotional_context": emotional_context,
            "dialogue_turns": len(dialogue_turns),
            "key_points": self._extract_key_points(transcript, entities)
        }

    def _split_dialogue(self, transcript: str) -> List[Dict]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏"""
        turns = []

        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = re.split(r'[.!?]+', transcript)
        for i, sent in enumerate(sentences):
            if sent.strip():
                # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤–æ–ø—Ä–æ—Å—ã –æ–±—ã—á–Ω–æ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞, –æ—Ç–≤–µ—Ç—ã –æ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä–∞
                speaker = "client" if '?' in sent else "manager"
                turns.append({
                    "speaker": speaker,
                    "text": sent.strip(),
                    "position": i
                })

        return turns

    def _extract_entities_natasha(self, text: str) -> Dict[str, List[Dict]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Natasha"""
        entities = {
            "persons": [],
            "organizations": [],
            "locations": [],
            "dates": [],
            "money": [],
            "phones": [],
            "emails": []
        }

        try:
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç Natasha
            doc = Doc(text)

            # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
            doc.segment(self.segmenter)

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è
            doc.tag_morph(self.morph_tagger)

            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
            for token in doc.tokens:
                token.lemmatize(self.morph_vocab)

            # NER
            doc.tag_ner(self.ner_tagger)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ doc.spans
            for span in doc.spans:
                entity_info = {
                    "text": text[span.start:span.stop],
                    "type": span.type,
                    "start": span.start,
                    "stop": span.stop
                }

                # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                if span.type == "PER":
                    entities["persons"].append(entity_info)
                elif span.type == "ORG":
                    entities["organizations"].append(entity_info)
                elif span.type == "LOC":
                    entities["locations"].append(entity_info)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã
            try:
                dates_matches = self.dates_extractor(text)
                for match in dates_matches:
                    # –£ match –µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã start –∏ stop –Ω–∞–ø—Ä—è–º—É—é
                    date_info = {
                        "text": text[match.start:match.stop],
                        "start": match.start,
                        "stop": match.stop
                    }
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                    if hasattr(match, 'fact') and match.fact:
                        date_info["fact"] = str(match.fact)
                    entities["dates"].append(date_info)
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—ã: {e}")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã
            try:
                money_matches = self.money_extractor(text)
                for match in money_matches:
                    # –£ match –µ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã start –∏ stop –Ω–∞–ø—Ä—è–º—É—é
                    money_info = {
                        "text": text[match.start:match.stop],
                        "start": match.start,
                        "stop": match.stop
                    }
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                    if hasattr(match, 'fact') and match.fact:
                        if hasattr(match.fact, 'amount'):
                            money_info["amount"] = match.fact.amount
                        if hasattr(match.fact, 'currency'):
                            money_info["currency"] = match.fact.currency
                        else:
                            money_info["currency"] = "RUB"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É–±–ª–∏
                    entities["money"].append(money_info)
            except Exception as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—É–º–º—ã: {e}")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
            phone_pattern = r'\+?[78]?\s?[\(\[]?\d{3}[\)\]]?\s?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}'
            for phone_match in re.finditer(phone_pattern, text):
                entities["phones"].append({
                    "text": phone_match.group(),
                    "start": phone_match.start(),
                    "stop": phone_match.end()
                })

            # –ò–∑–≤–ª–µ–∫–∞–µ–º email
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            for email_match in re.finditer(email_pattern, text):
                entities["emails"].append({
                    "text": email_match.group(),
                    "start": email_match.start(),
                    "stop": email_match.end()
                })

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–∏—è —Å—É—â–Ω–æ—Å—Ç–∏
            sanatorium_entities = self._extract_sanatorium_entities(text)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Å—É—â–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ None –∏ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∞–º–∏
            for key, value in sanatorium_entities.items():
                if isinstance(value, list):
                    entities[key] = value
                elif value is not None:
                    # –î–ª—è —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (duration_nights, guest_count) —Å–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏
                    entities[key] = value

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            total_count = 0
            for key, value in entities.items():
                if isinstance(value, list):
                    total_count += len(value)
                elif value is not None and not isinstance(value, dict):
                    total_count += 1

            logger.debug(f"Natasha –∏–∑–≤–ª–µ–∫–ª–∞ {total_count} —Å—É—â–Ω–æ—Å—Ç–µ–π")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π Natasha: {e}")
            import traceback
            traceback.print_exc()

        return entities

    def _extract_sanatorium_entities(self, text: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è —Å–∞–Ω–∞—Ç–æ—Ä–∏—è"""
        entities = {
            "room_types": [],
            "meal_types": [],
            "treatment_types": [],
            "duration_nights": None,
            "guest_count": None
        }

        # –¢–∏–ø—ã –Ω–æ–º–µ—Ä–æ–≤
        room_patterns = [
            r'(—Å—Ç–∞–Ω–¥–∞—Ä—Ç|–ª—é–∫—Å|–¥–µ–ª—é–∫—Å|—ç–∫–æ–Ω–æ–º|–æ–¥–Ω–æ–º–µ—Å—Ç–Ω—ã–π|–¥–≤—É—Ö–º–µ—Å—Ç–Ω—ã–π)',
            r'–Ω–æ–º–µ—Ä\s+(—Å –±–∞–ª–∫–æ–Ω–æ–º|–±–µ–∑ –±–∞–ª–∫–æ–Ω–∞|—Å –≤–∏–¥–æ–º –Ω–∞)',
        ]
        for pattern in room_patterns:
            matches = re.findall(pattern, text.lower())
            entities["room_types"].extend(matches)

        # –¢–∏–ø—ã –ø–∏—Ç–∞–Ω–∏—è
        meal_patterns = [
            r'(–∑–∞–≤—Ç—Ä–∞–∫|–æ–±–µ–¥|—É–∂–∏–Ω|—à–≤–µ–¥—Å–∫–∏–π —Å—Ç–æ–ª|–¥–∏–µ—Ç–∏—á–µ—Å–∫–æ–µ|—Ç—Ä–µ—Ö—Ä–∞–∑–æ–≤–æ–µ|–ø–∏—Ç–∞–Ω–∏–µ)',
            r'(–≤—Å–µ –≤–∫–ª—é—á–µ–Ω–æ|–ø–æ–ª–Ω—ã–π –ø–∞–Ω—Å–∏–æ–Ω|–ø–æ–ª—É–ø–∞–Ω—Å–∏–æ–Ω)'
        ]
        for pattern in meal_patterns:
            matches = re.findall(pattern, text.lower())
            entities["meal_types"].extend(matches)

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ—á–µ–π
        nights_pattern = r'(\d+)\s*(–Ω–æ—á–µ–π|–Ω–æ—á–∏|–Ω–æ—á—å|–¥–Ω–µ–π|–¥–Ω—è|–¥–µ–Ω—å|—Å—É—Ç–æ–∫)'
        nights_match = re.search(nights_pattern, text.lower())
        if nights_match:
            entities["duration_nights"] = int(nights_match.group(1))

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Å—Ç–µ–π
        guest_patterns = [
            r'(\d+)\s*(—á–µ–ª–æ–≤–µ–∫|–≤–∑—Ä–æ—Å–ª—ã—Ö|–¥–µ—Ç–µ–π|—Ä–µ–±–µ–Ω–æ–∫)',
            r'(–æ–¥–∏–Ω|–æ–¥–Ω–∞|–¥–≤–æ–µ|—Ç—Ä–æ–µ|—á–µ—Ç–≤–µ—Ä–æ)\s*(—á–µ–ª–æ–≤–µ–∫|–≤–∑—Ä–æ—Å–ª—ã—Ö)?'
        ]
        for pattern in guest_patterns:
            match = re.search(pattern, text.lower())
            if match:
                number_text = match.group(1)
                text_to_num = {
                    '–æ–¥–∏–Ω': 1, '–æ–¥–Ω–∞': 1, '–¥–≤–æ–µ': 2, '—Ç—Ä–æ–µ': 3, '—á–µ—Ç–≤–µ—Ä–æ': 4
                }
                if number_text in text_to_num:
                    entities["guest_count"] = text_to_num[number_text]
                elif number_text.isdigit():
                    entities["guest_count"] = int(number_text)
                break

        return entities

    def _analyze_intents_fallback(self, dialogue_turns: List[Dict]) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        intents = []

        intent_patterns = {
            "–∑–∞–ø—Ä–æ—Å_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏": ["—Å–∫–æ–ª—å–∫–æ", "–∫–∞–∫–∞—è —Ü–µ–Ω–∞", "—á—Ç–æ –≤—Ö–æ–¥–∏—Ç", "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ"],
            "–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞—Ç—å", "–æ—Ñ–æ—Ä–º–∏—Ç—å"],
            "–≤–æ–∑—Ä–∞–∂–µ–Ω–∏–µ": ["–¥–æ—Ä–æ–≥–æ", "–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç", "–ø–æ–¥—É–º–∞—Ç—å", "–Ω–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç"],
            "—Å–æ–≥–ª–∞—Å–∏–µ": ["—Ö–æ—Ä–æ—à–æ", "–ø–æ–¥—Ö–æ–¥–∏—Ç", "—É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç", "–¥–∞–≤–∞–π—Ç–µ"],
            "—É—Ç–æ—á–Ω–µ–Ω–∏–µ": ["–∞ —á—Ç–æ –µ—Å–ª–∏", "–º–æ–∂–Ω–æ –ª–∏", "–≤–æ–∑–º–æ–∂–Ω–æ –ª–∏"],
            "–æ—Ç–∫–∞–∑": ["–Ω–µ –±—É–¥—É", "–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—Å–ø–∞—Å–∏–±–æ, –Ω–µ—Ç"],
            "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å": ["—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é"]
        }

        for turn in dialogue_turns:
            text_lower = turn["text"].lower()
            detected_intent = "–æ–±—â–∏–π"
            max_confidence = 0.3

            for intent_name, keywords in intent_patterns.items():
                matches = sum(1 for keyword in keywords if keyword in text_lower)
                confidence = matches / len(keywords) if keywords else 0

                if confidence > max_confidence:
                    detected_intent = intent_name
                    max_confidence = confidence

            intents.append({
                "speaker": turn["speaker"],
                "text": turn["text"][:100],
                "intent": detected_intent,
                "confidence": max_confidence
            })

        return intents

    def _determine_topic(self, transcript: str, intents: List[Dict]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–º—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        text_lower = transcript.lower()

        # –ü–æ–¥—Å—á–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–∞–º
        topic_scores = {}

        for topic, keywords in self.sanatorium_topics.items():
            score = sum(3 if keyword in text_lower else 0 for keyword in keywords)
            if score > 0:
                topic_scores[topic] = score

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–º—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—á–µ—Ç–æ–º
        if topic_scores:
            return max(topic_scores, key=topic_scores.get)

        return "–û–±—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä"

    def _find_objections(self, transcript: str, dialogue_turns: List[Dict]) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π"""
        objections = []
        text_lower = transcript.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è —Å–∞–Ω–∞—Ç–æ—Ä–∏—è
        for objection_type, objection_data in self.sanatorium_objections.items():
            for pattern in objection_data["patterns"]:
                matches = re.finditer(pattern, text_lower)
                for match in matches:
                    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    start = max(0, match.start() - 100)
                    end = min(len(text_lower), match.end() + 100)
                    context = transcript[start:end]

                    objections.append({
                        "type": objection_type,
                        "text": match.group(),
                        "context": context,
                        "recommendation": objection_data["recommendation"],
                        "confidence": 0.9
                    })

        return objections

    def _determine_sales_stage(self, dialogue_turns: List[Dict], intents: List[Dict]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç—Ç–∞–ø–∞ –ø—Ä–æ–¥–∞–∂–∏"""
        if len(dialogue_turns) < 3:
            return self.sales_stages["initial_contact"]

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        full_text = " ".join([turn["text"] for turn in dialogue_turns])
        full_text_lower = full_text.lower()

        if "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å" in full_text_lower or "–æ—Ñ–æ—Ä–º–∏—Ç—å" in full_text_lower:
            return self.sales_stages["closing"]
        elif "–≤–æ–∑—Ä–∞–∂–µ–Ω–∏–µ" in [intent["intent"] for intent in intents]:
            return self.sales_stages["objection_handling"]
        elif "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ" in full_text_lower or "—á—Ç–æ –≤—Ö–æ–¥–∏—Ç" in full_text_lower:
            return self.sales_stages["presentation"]
        elif "–∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç" in full_text_lower or "—Ö–æ—Ç–µ–ª —É–∑–Ω–∞—Ç—å" in full_text_lower:
            return self.sales_stages["needs_identification"]

        return self.sales_stages["needs_identification"]

    def _analyze_call_result(self, transcript: str, dialogue_turns: List[Dict], objections: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–≤–æ–Ω–∫–∞"""
        text_lower = transcript.lower()

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Å–ø–µ—à–Ω–æ–π –ø—Ä–æ–¥–∞–∂–∏
        success_indicators = [
            "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–ª–∏", "–æ—Ñ–æ—Ä–º–∏–ª–∏", "–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å", "–∂–¥—É –≤–∞—Å",
            "–¥–æ –≤—Å—Ç—Ä–µ—á–∏", "–æ–ø–ª–∞—Ç–∏–ª", "–≤–Ω–µ—Å–ª–∏ –ø—Ä–µ–¥–æ–ø–ª–∞—Ç—É", "—Å—á–µ—Ç –≤—ã—Å—Ç–∞–≤–ª–µ–Ω"
        ]

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –æ—Ç–∫–∞–∑–∞
        failure_indicators = [
            "–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç", "–¥–æ—Ä–æ–≥–æ", "–ø–æ–¥—É–º–∞–µ–º", "–ø–µ—Ä–µ–∑–≤–æ–Ω—é",
            "–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—Å–ø–∞—Å–∏–±–æ, –Ω–µ—Ç", "–≤ –¥—Ä—É–≥–æ–π —Ä–∞–∑"
        ]

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ä–∞–±–æ—Ç—ã
        followup_indicators = [
            "–ø—Ä–∏—à–ª–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é", "–æ—Ç–ø—Ä–∞–≤—å—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ",
            "–æ–±—Å—É–∂—É —Å", "–Ω—É–∂–Ω–æ –ø–æ–¥—É–º–∞—Ç—å", "–∏–∑—É—á—É"
        ]

        # –ü–æ–¥—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        success_score = sum(1 for ind in success_indicators if ind in text_lower)
        failure_score = sum(1 for ind in failure_indicators if ind in text_lower)
        followup_score = sum(1 for ind in followup_indicators if ind in text_lower)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_type = "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"
        confidence = 0.5

        if success_score > failure_score and success_score > followup_score:
            result_type = "—É—Å–ø–µ—à–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞"
            confidence = min(0.9, success_score * 0.3)
        elif failure_score > success_score:
            result_type = "–æ—Ç–∫–∞–∑"
            confidence = min(0.9, failure_score * 0.3)
        elif followup_score > 0:
            result_type = "—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞"
            confidence = min(0.8, followup_score * 0.4)

        return {
            "result": result_type,
            "confidence": confidence,
            "success_indicators_found": success_score,
            "failure_indicators_found": failure_score,
            "followup_needed": followup_score > 0,
            "objections_count": len(objections),
            "recommendation": self._get_result_recommendation(result_type, objections)
        }

    def _get_result_recommendation(self, result_type: str, objections: List[Dict]) -> str:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∑–≤–æ–Ω–∫–∞"""
        if result_type == "—É—Å–ø–µ—à–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞":
            return "–û—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –±—Ä–æ–Ω–∏ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞"
        elif result_type == "–æ—Ç–∫–∞–∑":
            if objections:
                main_objection = objections[0]["type"] if objections else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                return f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω—É –æ—Ç–∫–∞–∑–∞: {main_objection}. –í–æ–∑–º–æ–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∫–æ–Ω—Ç–∞–∫—Ç —á–µ—Ä–µ–∑ 2-3 –Ω–µ–¥–µ–ª–∏"
            return "–í—ã—è—Å–Ω–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—É—é –ø—Ä–∏—á–∏–Ω—É –æ—Ç–∫–∞–∑–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–æ–≤"
        elif result_type == "—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞":
            return "–û—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –Ω–∞–∑–Ω–∞—á–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–≤–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ 2-3 –¥–Ω—è"
        else:
            return "–£—Ç–æ—á–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —É –º–µ–Ω–µ–¥–∂–µ—Ä–∞"

    def _extract_business_data(self, entities: Dict, transcript: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –±–∏–∑–Ω–µ—Å-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        business_data = {
            "dates": {
                "check_in": None,
                "check_out": None,
                "duration": entities.get("duration_nights")
            },
            "guests": {
                "adults": None,
                "children": None,
                "total": entities.get("guest_count")
            },
            "preferences": {
                "room_type": entities.get("room_types", []),
                "meal_plan": entities.get("meal_types", []),
                "treatments": entities.get("treatment_types", [])
            },
            "financial": {
                "quoted_prices": entities.get("money", []),
                "discount_mentioned": self._check_discount(transcript),
                "payment_method": self._extract_payment_method(transcript)
            },
            "contact_info": {
                "phones": entities.get("phones", []),
                "emails": entities.get("emails", []),
                "names": entities.get("persons", [])
            }
        }

        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–æ—Å—Ç–µ–π
        adults_pattern = r'(\d+)\s*–≤–∑—Ä–æ—Å–ª—ã—Ö'
        children_pattern = r'(\d+)\s*(—Ä–µ–±–µ–Ω|–¥–µ—Ç)'

        adults_match = re.search(adults_pattern, transcript.lower())
        if adults_match:
            business_data["guests"]["adults"] = int(adults_match.group(1))

        children_match = re.search(children_pattern, transcript.lower())
        if children_match:
            business_data["guests"]["children"] = int(children_match.group(1))

        return business_data

    def _check_discount(self, transcript: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å–∫–∏–¥–æ–∫"""
        text_lower = transcript.lower()
        discount_info = {
            "mentioned": False,
            "percentage": None,
            "type": None
        }

        # –ü–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ —Å–∫–∏–¥–∫–∏
        percent_pattern = r'(\d+)\s*(%|–ø—Ä–æ—Ü–µ–Ω—Ç)'
        percent_match = re.search(percent_pattern, text_lower)

        if percent_match and "—Å–∫–∏–¥–∫" in text_lower:
            discount_info["mentioned"] = True
            discount_info["percentage"] = int(percent_match.group(1))

        # –¢–∏–ø—ã —Å–∫–∏–¥–æ–∫
        if "–∞–∫—Ü–∏" in text_lower:
            discount_info["type"] = "–∞–∫—Ü–∏—è"
        elif "—Ä–∞–Ω–Ω–µ–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ" in text_lower:
            discount_info["type"] = "—Ä–∞–Ω–Ω–µ–µ –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"
        elif "–ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç" in text_lower:
            discount_info["type"] = "–ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç"

        return discount_info

    def _extract_payment_method(self, transcript: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–∞ –æ–ø–ª–∞—Ç—ã"""
        text_lower = transcript.lower()

        if "–Ω–∞–ª–∏—á–Ω—ã" in text_lower:
            return "–Ω–∞–ª–∏—á–Ω—ã–µ"
        elif "–∫–∞—Ä—Ç" in text_lower:
            return "–∫–∞—Ä—Ç–∞"
        elif "–±–µ–∑–Ω–∞–ª" in text_lower or "–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ" in text_lower:
            return "–±–µ–∑–Ω–∞–ª–∏—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç"
        elif "—Ä–∞—Å—Å—Ä–æ—á–∫" in text_lower:
            return "—Ä–∞—Å—Å—Ä–æ—á–∫–∞"
        elif "–ø—Ä–µ–¥–æ–ø–ª–∞—Ç" in text_lower:
            return "–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞"

        return None

    def _analyze_emotional_context(self, dialogue_turns: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞"""
        emotional_context = {
            "client_mood": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
            "manager_mood": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
            "tension_points": [],
            "positive_moments": []
        }

        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —ç–º–æ—Ü–∏–π
        positive_indicators = ["—Å–ø–∞—Å–∏–±–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ"]
        negative_indicators = ["–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–¥–æ—Ä–æ–≥–æ", "–Ω–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç", "–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç"]

        for i, turn in enumerate(dialogue_turns):
            text_lower = turn["text"].lower()

            # –ü–æ–¥—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            positive_score = sum(1 for ind in positive_indicators if ind in text_lower)
            negative_score = sum(1 for ind in negative_indicators if ind in text_lower)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            if positive_score > negative_score:
                emotional_context["positive_moments"].append({
                    "turn": i,
                    "text": turn["text"][:100]
                })
            elif negative_score > positive_score:
                emotional_context["tension_points"].append({
                    "turn": i,
                    "text": turn["text"][:100],
                    "severity": "high" if negative_score > 2 else "medium"
                })

        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        if len(emotional_context["positive_moments"]) > len(emotional_context["tension_points"]):
            emotional_context["client_mood"] = "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
        elif len(emotional_context["tension_points"]) > 2:
            emotional_context["client_mood"] = "–Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—ã–π"

        return emotional_context

    def _extract_key_points(self, transcript: str, entities: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        key_points = []
        sentences = re.split(r'[.!?]+', transcript)

        # –í–∞–∂–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        important_indicators = [
            "–≥–ª–∞–≤–Ω–æ–µ", "–≤–∞–∂–Ω–æ", "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ", "–∫—Ä–∏—Ç–∏—á–Ω–æ",
            "–ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ", "–æ—Å–Ω–æ–≤–Ω–æ–µ", "–∫–ª—é—á–µ–≤–æ–µ"
        ]

        # –ë–∏–∑–Ω–µ—Å-–∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        business_critical = [
            "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–ª–∏", "–æ–ø–ª–∞—Ç–∏–ª", "–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å",
            "–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç", "–æ—Ç–∫–∞–∑—ã–≤–∞—é—Å—å", "–¥–æ—Ä–æ–≥–æ",
            "–Ω—É–∂–Ω–æ –ø–æ–¥—É–º–∞—Ç—å", "–ø–µ—Ä–µ–∑–≤–æ–Ω—é"
        ]

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence or len(sentence) < 20:
                continue

            sentence_lower = sentence.lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å
            is_important = any(ind in sentence_lower for ind in important_indicators)
            is_business_critical = any(phrase in sentence_lower for phrase in business_critical)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤–∞–∂–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
            has_money = any(money.get("text", "") in sentence for money in entities.get("money", []))
            has_dates = any(date.get("text", "") in sentence for date in entities.get("dates", []))

            if is_important or is_business_critical or has_money or has_dates:
                key_points.append(sentence)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return key_points[:5]